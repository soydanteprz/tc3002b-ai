import os
import pandas as pd
import javalang
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from difflib import SequenceMatcher
import re

class ASTNormalizer:
    """Normaliza cÃ³digo Java usando el AST para eliminar cambios superficiales"""

    def __init__(self):
        self.var_counter = 0
        self.method_counter = 0
        self.class_counter = 0
        self.param_counter = 0

        self.var_map = {}
        self.method_map = {}
        self.class_map = {}
        self.param_map = {}

    def reset_counters(self):
        """Reinicia los contadores para cada archivo"""
        self.var_counter = 0
        self.method_counter = 0
        self.class_counter = 0
        self.param_counter = 0

        self.var_map = {}
        self.method_map = {}
        self.class_map = {}
        self.param_map = {}

    def normalize_code(self, code):
        """Normaliza el cÃ³digo reemplazando identificadores"""
        if not code or len(code.strip()) == 0:
            return code

        # Pre-process code
        code = code.replace('\r\n', '\n').replace('\t', '    ')

        try:
            self.reset_counters()
            tree = javalang.parse.parse(code)

            # Primero, recolectar todos los identificadores
            self._collect_identifiers(tree)

            # Luego, reemplazar en el cÃ³digo original
            normalized = self._replace_identifiers(code)

            return normalized
        except javalang.parser.JavaSyntaxError as se:
            # Extract meaningful error details
            error_msg = str(se)
            line_info = f"at line {se.at.line}" if hasattr(se, 'at') and hasattr(se.at, 'line') else ""
            print(f"Error normalizando cÃ³digo: Syntax error {line_info} - {error_msg}")
            return code
        except javalang.tokenizer.LexerError as le:
            print(f"Error normalizando cÃ³digo: Lexer error - {str(le)}")
            return code
        except Exception as e:
            print(f"Error normalizando cÃ³digo: {str(e)}")
            return code

    def _collect_identifiers(self, tree):
        """Recolecta todos los identificadores del AST"""
        # Clases
        for path, node in tree.filter(javalang.tree.ClassDeclaration):
            if node.name not in self.class_map:
                self.class_map[node.name] = f"CLASS{self.class_counter}"
                self.class_counter += 1

        # MÃ©todos
        for path, node in tree.filter(javalang.tree.MethodDeclaration):
            if node.name not in self.method_map and node.name != 'main':
                self.method_map[node.name] = f"METHOD{self.method_counter}"
                self.method_counter += 1

        # ParÃ¡metros
        for path, node in tree.filter(javalang.tree.FormalParameter):
            if node.name not in self.param_map:
                self.param_map[node.name] = f"PARAM{self.param_counter}"
                self.param_counter += 1

        # Variables locales
        for path, node in tree.filter(javalang.tree.LocalVariableDeclaration):
            for declarator in node.declarators:
                if declarator.name not in self.var_map:
                    self.var_map[declarator.name] = f"VAR{self.var_counter}"
                    self.var_counter += 1

        # Variables de campo
        for path, node in tree.filter(javalang.tree.FieldDeclaration):
            for declarator in node.declarators:
                if declarator.name not in self.var_map:
                    self.var_map[declarator.name] = f"FIELD{self.var_counter}"
                    self.var_counter += 1

    def _replace_identifiers(self, code):
        """Reemplaza los identificadores en el cÃ³digo"""
        # Crear un mapa combinado con prioridad
        all_mappings = {}
        all_mappings.update(self.var_map)
        all_mappings.update(self.param_map)
        all_mappings.update(self.method_map)
        all_mappings.update(self.class_map)

        # Ordenar por longitud descendente para evitar reemplazos parciales
        sorted_mappings = sorted(all_mappings.items(), key=lambda x: len(x[0]), reverse=True)

        normalized = code
        for original, replacement in sorted_mappings:
            # Usar word boundaries para evitar reemplazos parciales
            pattern = r'\b' + re.escape(original) + r'\b'
            normalized = re.sub(pattern, replacement, normalized)

        return normalized


class ASTStructureAnalyzer:
    """Analiza la estructura del AST para comparaciÃ³n"""

    @staticmethod
    def extract_ast_structure(code):
        """Extrae la secuencia de estructura del AST"""
        if not code or len(code.strip()) == 0:
            return []

        # Pre-process code to handle potential syntax issues
        code = code.replace('\r\n', '\n').replace('\t', '    ')

        # Additional preprocessing to handle common problems
        # Remove package declarations which might cause issues
        code = re.sub(r'^package\s+[\w.]+;', '', code)

        # Ensure class structure is present
        if '{' not in code or '}' not in code:
            if 'class' not in code.lower():
                # Try to wrap code in a dummy class if it's just a fragment
                code = "class DummyWrapper {\n" + code + "\n}"

        try:
            # Try parsing with javalang
            tree = javalang.parse.parse(code)
            structure = []

            # Traverse AST
            for path, node in tree:
                node_type = type(node).__name__

                # Add additional information based on node type
                if isinstance(node, javalang.tree.MethodDeclaration):
                    param_count = len(node.parameters) if node.parameters else 0
                    structure.append(f"{node_type}_{param_count}")
                elif isinstance(node, javalang.tree.ForStatement):
                    structure.append(f"{node_type}_LOOP")
                elif isinstance(node, javalang.tree.WhileStatement):
                    structure.append(f"{node_type}_LOOP")
                elif isinstance(node, javalang.tree.IfStatement):
                    has_else = 1 if node.else_statement else 0
                    structure.append(f"{node_type}_{has_else}")
                elif isinstance(node, javalang.tree.BinaryOperation):
                    structure.append(f"{node_type}_{node.operator}")
                else:
                    structure.append(node_type)

            return structure

        except javalang.parser.JavaSyntaxError as se:
            # Extract meaningful error details
            error_msg = str(se)
            line_info = f"at line {se.at.line}" if hasattr(se, 'at') and hasattr(se.at, 'line') else ""
            print(f"Error extrayendo estructura AST: Syntax error {line_info} - {error_msg}")
            return []
        except javalang.tokenizer.LexerError as le:
            print(f"Error extrayendo estructura AST: Lexer error - {str(le)}")
            return []
        except Exception as e:
            print(f"Error extrayendo estructura AST: {str(e)}")
            return []

    @staticmethod
    def extract_method_signatures(code):
        """Extrae las firmas de los mÃ©todos normalizadas"""
        signatures = []
        try:
            tree = javalang.parse.parse(code)

            for path, node in tree.filter(javalang.tree.MethodDeclaration):
                # Crear firma normalizada
                param_types = []
                if node.parameters:
                    for param in node.parameters:
                        param_type = param.type.name if hasattr(param.type, 'name') else str(param.type)
                        param_types.append(param_type)

                signature = f"METHOD({','.join(param_types)})"
                signatures.append(signature)

        except:
            pass

        return signatures

    def debug_file_parsing(self, filepath):
        """Helper method to debug parsing issues with specific files"""
        print(f"\nðŸ“‹ Debugging file: {filepath}")
        try:
            code = self.read_java_file(filepath)
            print(f"File size: {len(code)} bytes")
            print(f"Is empty: {len(code.strip()) == 0}")

            # Check basic content
            first_100_chars = code[:100].replace('\n', '\\n')
            print(f"Content starts with: {first_100_chars}...")

            # Try to parse
            print("Attempting to parse with javalang...")
            tree = javalang.parse.parse(code)
            print("âœ… Parsing successful!")

            # Count nodes
            node_count = 0
            for _, _ in tree:
                node_count += 1
            print(f"AST contains {node_count} nodes")

            return True
        except Exception as e:
            print(f"âŒ Parsing failed: {e}")
            if "at position" in str(e):
                position = str(e).split("at position")[1].strip()
                print(f"Error position: {position}")

                # Show problematic code around the error position
                try:
                    position = int(position)
                    start = max(0, position - 50)
                    end = min(len(code), position + 50)
                    print(f"Code around error position:\n```\n{code[start:end]}\n```")
                except:
                    pass
            return False

    @staticmethod
    def calculate_lcs(seq1, seq2):
        """Calcula la subsecuencia comÃºn mÃ¡s larga"""
        m, n = len(seq1), len(seq2)

        # Crear matriz DP
        dp = [[0] * (n + 1) for _ in range(m + 1)]

        # Llenar la matriz
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if seq1[i-1] == seq2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])

        return dp[m][n]

    @staticmethod
    def structure_similarity(code1, code2):
        """Calcula la similitud estructural entre dos cÃ³digos"""
        struct1 = ASTStructureAnalyzer.extract_ast_structure(code1)
        struct2 = ASTStructureAnalyzer.extract_ast_structure(code2)

        if not struct1 or not struct2:
            return 0.0

        # LCS normalizado
        lcs_length = ASTStructureAnalyzer.calculate_lcs(struct1, struct2)
        similarity = 2 * lcs_length / (len(struct1) + len(struct2))

        return similarity

    @staticmethod
    def method_signature_similarity(code1, code2):
        """Compara las firmas de mÃ©todos entre dos cÃ³digos"""
        sigs1 = set(ASTStructureAnalyzer.extract_method_signatures(code1))
        sigs2 = set(ASTStructureAnalyzer.extract_method_signatures(code2))

        if not sigs1 and not sigs2:
            return 1.0  # Ambos sin mÃ©todos
        if not sigs1 or not sigs2:
            return 0.0

        # Jaccard similarity
        intersection = len(sigs1 & sigs2)
        union = len(sigs1 | sigs2)

        return intersection / union if union > 0 else 0.0


class EnhancedPlagiarismDetector:
    """Detector de plagio mejorado con normalizaciÃ³n AST"""

    def __init__(self):
        self.normalizer = ASTNormalizer()
        self.structure_analyzer = ASTStructureAnalyzer()

    def read_java_file(self, filepath):
        with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:
            return f.read()

    def clean_code(self, code):
        # Eliminar comentarios
        code = re.sub(r'//.*?\n', '\n', code)
        code = re.sub(r'/\*.*?\*/', '', code, flags=re.DOTALL)
        # Eliminar espacios excesivos
        code = re.sub(r'\s+', ' ', code).strip()
        return code

    def tokenize_code(self, code):
        try:
            tokens = list(javalang.tokenizer.tokenize(code))
            return ' '.join(token.value for token in tokens)
        except:
            return ''

    def calculate_similarity(self, file1_path, file2_path):
        """Calcula mÃºltiples mÃ©tricas de similitud"""
        try:
            code1_raw = self.read_java_file(file1_path)
            code2_raw = self.read_java_file(file2_path)

            # Limpiar cÃ³digo
            code1_clean = self.clean_code(code1_raw)
            code2_clean = self.clean_code(code2_raw)

            # Normalizar con AST
            code1_normalized = self.normalizer.normalize_code(code1_clean)
            code2_normalized = self.normalizer.normalize_code(code2_clean)

            # Tokenizar para TF-IDF
            tokens1 = self.tokenize_code(code1_normalized)
            tokens2 = self.tokenize_code(code2_normalized)

            results = {}

            # 1. TF-IDF con cÃ³digo normalizado
            if tokens1 and tokens2:
                vectorizer = TfidfVectorizer()
                tfidf_matrix = vectorizer.fit_transform([tokens1, tokens2])
                results['tfidf_normalized'] = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]
            else:
                results['tfidf_normalized'] = 0.0

            # 2. Similitud estructural AST
            results['ast_structure'] = self.structure_analyzer.structure_similarity(code1_clean, code2_clean)

            # 3. Similitud de firmas de mÃ©todos
            results['method_signatures'] = self.structure_analyzer.method_signature_similarity(code1_clean, code2_clean)

            # 4. Similitud de secuencia (sobre cÃ³digo normalizado)
            results['sequence_normalized'] = SequenceMatcher(None, code1_normalized, code2_normalized).ratio()

            # 5. PuntuaciÃ³n combinada (ponderada)
            weights = {
                'tfidf_normalized': 0.35,
                'ast_structure': 0.30,
                'method_signatures': 0.20,
                'sequence_normalized': 0.15
            }

            results['combined_score'] = sum(results[metric] * weights[metric] for metric in weights)
            return results

        except Exception as e:
            print(f"Error calculando similitud entre {file1_path} y {file2_path}: {e}")
            return {
                'tfidf_normalized': 0.0,
                'ast_structure': 0.0,
                'method_signatures': 0.0,
                'sequence_normalized': 0.0,
                'combined_score': 0.0
            }

    def debug_file_parsing(self, filepath):
        """Helper method to debug parsing issues with specific files"""
        print(f"\nðŸ“‹ Debugging file: {filepath}")
        try:
            code = self.read_java_file(filepath)
            print(f"File size: {len(code)} bytes")
            print(f"Is empty: {len(code.strip()) == 0}")

            # Check basic content
            first_100_chars = code[:100].replace('\n', '\\n')
            print(f"Content starts with: {first_100_chars}...")

            # Try to parse
            print("Attempting to parse with javalang...")
            tree = javalang.parse.parse(code)
            print("âœ… Parsing successful!")

            # Count nodes
            node_count = 0
            for _, _ in tree:
                node_count += 1
            print(f"AST contains {node_count} nodes")

            return True
        except Exception as e:
            print(f"âŒ Parsing failed: {e}")
            if "at position" in str(e):
                position = str(e).split("at position")[1].strip()
                print(f"Error position: {position}")

                # Show problematic code around the error position
                try:
                    position = int(position)
                    start = max(0, position - 50)
                    end = min(len(code), position + 50)
                    print(f"Code around error position:\n```\n{code[start:end]}\n```")
                except:
                    pass
            return False

    def process_dataset(self, base_path, csv_path, output_csv='similarity_ast_enhanced.csv', verbose=False):
        """Procesa el dataset completo con las nuevas mÃ©tricas"""
        df = pd.read_csv(csv_path)
        df = df[df['source_dataset'].isin(['ir_plag', 'conplag'])]

        resultados = []
        total_files = len(df)
        print(f"\nðŸ” Procesando {total_files} pares de archivos...")

        for idx, row in df.iterrows():
            # Solo mostrar progreso si verbose=True
            if verbose and idx % 50 == 0 and idx > 0:
                print(f"  â–¶ {idx}/{total_files} completados ({idx/total_files*100:.1f}%)")

            dataset = row['source_dataset']
            plagio = row['label']
            file1 = row['file1']
            file2 = row['file2']
            file1_base = os.path.splitext(file1)[0]
            file2_base = os.path.splitext(file2)[0]

            # Construir paths segÃºn el dataset
            if dataset == 'ir_plag':
                folder = row['folder_name']
                folder_path = os.path.join(base_path, folder)
                path1 = os.path.join(folder_path, 'original.java')
                path2 = os.path.join(folder_path, 'compared.java')

            elif dataset == 'conplag':
                folder1 = os.path.join(base_path, f"{file1_base}_{file2_base}")
                folder2 = os.path.join(base_path, f"{file2_base}_{file1_base}")

                if os.path.isdir(folder1):
                    folder_path = folder1
                elif os.path.isdir(folder2):
                    folder_path = folder2
                else:
                    print(f"[âŒ] Carpeta no encontrada para {file1} y {file2}")
                    continue

                path1 = os.path.join(folder_path, file1)
                path2 = os.path.join(folder_path, file2)

            else:
                continue

            if not (os.path.exists(path1) and os.path.exists(path2)):
                print(f"[âš ï¸] Archivos no encontrados: {path1}, {path2}")
                continue

            try:
                # Calcular todas las mÃ©tricas
                metrics = self.calculate_similarity(path1, path2)

                resultado = {
                    'folder': os.path.basename(folder_path),
                    'file1': os.path.basename(path1),
                    'file2': os.path.basename(path2),
                    'es_plagio': plagio,
                    'dataset': dataset,
                    **metrics  # Agregar todas las mÃ©tricas
                }

                resultados.append(resultado)

            except Exception as e:
                # Solo mostrar error si verbose=True
                if verbose:
                    print(f"[âŒ] Error procesando {folder_path}: {e}")
                continue

        # Guardar resultados
        df_resultados = pd.DataFrame(resultados)
        df_resultados.to_csv(output_csv, index=False)

        print(f"\nâœ… Procesamiento completado: {len(resultados)}/{total_files} archivos")

        # Mostrar estadÃ­sticas
        print("\nðŸ“Š EstadÃ­sticas de resultados:")
        print("-" * 60)

        # EstadÃ­sticas por dataset y tipo
        stats = df_resultados.groupby(['dataset', 'es_plagio'])['combined_score'].agg(['mean', 'std', 'count'])
        print(stats.round(3))

        # EstadÃ­sticas generales
        print("\nðŸ“ˆ Resumen general:")
        print(f"  â€¢ Total de archivos procesados: {len(df_resultados)}")
        print(f"  â€¢ Casos de plagio: {df_resultados['es_plagio'].sum()} ({df_resultados['es_plagio'].mean()*100:.1f}%)")
        print(f"  â€¢ Score promedio (plagio=1): {df_resultados[df_resultados['es_plagio']==1]['combined_score'].mean():.3f}")
        print(f"  â€¢ Score promedio (plagio=0): {df_resultados[df_resultados['es_plagio']==0]['combined_score'].mean():.3f}")

        # AnÃ¡lisis de separaciÃ³n
        plagiarized_scores = df_resultados[df_resultados['es_plagio']==1]['combined_score']
        not_plagiarized_scores = df_resultados[df_resultados['es_plagio']==0]['combined_score']

        # Calcular overlap
        threshold = 0.5
        false_positives = (not_plagiarized_scores > threshold).sum()
        false_negatives = (plagiarized_scores < threshold).sum()

        print(f"\nðŸŽ¯ AnÃ¡lisis con umbral={threshold}:")
        print(f"  â€¢ Falsos positivos: {false_positives} ({false_positives/len(not_plagiarized_scores)*100:.1f}%)")
        print(f"  â€¢ Falsos negativos: {false_negatives} ({false_negatives/len(plagiarized_scores)*100:.1f}%)")

        print(f"\nðŸ’¾ Resultados guardados en: {output_csv}")

        return df_resultados

    def find_problematic_files(self, base_path, csv_path):
        """Find and debug files that can't be parsed"""
        print("\nðŸ” Searching for problematic files...")
        df = pd.read_csv(csv_path)
        df = df[df['source_dataset'].isin(['ir_plag', 'conplag'])]

        problematic_files = []

        for idx, row in df.iterrows():
            if idx % 100 == 0:
                print(f"Checking file {idx}/{len(df)}...")

            dataset = row['source_dataset']
            file1 = row['file1']
            file2 = row['file2']
            file1_base = os.path.splitext(file1)[0]
            file2_base = os.path.splitext(file2)[0]

            # Construct paths as in process_dataset
            if dataset == 'ir_plag':
                folder = row['folder_name']
                folder_path = os.path.join(base_path, folder)
                path1 = os.path.join(folder_path, 'original.java')
                path2 = os.path.join(folder_path, 'compared.java')
            elif dataset == 'conplag':
                folder1 = os.path.join(base_path, f"{file1_base}_{file2_base}")
                folder2 = os.path.join(base_path, f"{file2_base}_{file1_base}")

                if os.path.isdir(folder1):
                    folder_path = folder1
                elif os.path.isdir(folder2):
                    folder_path = folder2
                else:
                    continue

                path1 = os.path.join(folder_path, file1)
                path2 = os.path.join(folder_path, file2)
            else:
                continue

            # Check if files can be parsed
            for path in [path1, path2]:
                if not os.path.exists(path):
                    continue

                try:
                    code = self.read_java_file(path)
                    javalang.parse.parse(code)
                except Exception as e:
                    problematic_files.append((path, str(e)))
                    print(f"ðŸ’¥ Found problematic file: {path}")
                    print(f"   Error: {str(e)}")

                    # Debug first few problematic files in detail
                    if len(problematic_files) <= 3:
                        self.debug_file_parsing(path)

        print(f"\nâœ… Found {len(problematic_files)} problematic files")
        return problematic_files


# FunciÃ³n principal para ejecutar
if __name__ == '__main__':
    BASE_PATH = 'data/splits/train'
    CSV_PATH = 'data/splits/train.csv'

    print("ðŸš€ DETECTOR DE PLAGIO CON NORMALIZACIÃ“N AST")
    print("=" * 60)

    detector = EnhancedPlagiarismDetector()
    df_results = detector.process_dataset(BASE_PATH, CSV_PATH)

    # AnÃ¡lisis adicional de mÃ©tricas individuales
    print("\nðŸ” AnÃ¡lisis detallado de mÃ©tricas:")
    print("-" * 60)
    metrics_cols = ['tfidf_normalized', 'ast_structure', 'method_signatures', 'sequence_normalized', 'combined_score']

    metrics_analysis = []
    for metric in metrics_cols:
        plag_mean = df_results[df_results['es_plagio']==1][metric].mean()
        no_plag_mean = df_results[df_results['es_plagio']==0][metric].mean()
        separation = plag_mean - no_plag_mean

        metrics_analysis.append({
            'MÃ©trica': metric,
            'Media (Plagio)': round(plag_mean, 3),
            'Media (No Plagio)': round(no_plag_mean, 3),
            'SeparaciÃ³n': round(separation, 3)
        })

    metrics_df = pd.DataFrame(metrics_analysis)
    print(metrics_df.to_string(index=False))

    # Identificar mejores mÃ©tricas
    best_metric = metrics_df.loc[metrics_df['SeparaciÃ³n'].idxmax()]
    print(f"\nâ­ Mejor mÃ©trica individual: {best_metric['MÃ©trica']} (separaciÃ³n: {best_metric['SeparaciÃ³n']})")

    print("\nâœ… AnÃ¡lisis completado!")
    print("\nðŸ’¡ Sugerencias:")
    print("  1. Ejecuta el anÃ¡lisis de visualizaciÃ³n para grÃ¡ficos detallados")
    print("  2. Considera ajustar los pesos de las mÃ©tricas segÃºn tu dataset")
    print("  3. Prueba con diferentes umbrales para optimizar precision/recall")
