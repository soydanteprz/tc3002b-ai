{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:03:40.284689Z",
     "start_time": "2025-06-03T16:03:40.281758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import javalang\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:03:40.297891Z",
     "start_time": "2025-06-03T16:03:40.293860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_java_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "def clean_code(code):\n",
    "    import re\n",
    "    code = re.sub(r'//.*?\\n', '\\n', code)\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    code = re.sub(r'\\s+', ' ', code).strip()\n",
    "    return code\n",
    "\n",
    "def tokenize_code(code):\n",
    "    try:\n",
    "        tokens = list(javalang.tokenizer.tokenize(code))\n",
    "        return ' '.join(token.value for token in tokens)\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def preprocess_file(filepath):\n",
    "    raw = read_java_file(filepath)\n",
    "    cleaned = clean_code(raw)\n",
    "    tokens = tokenize_code(cleaned)\n",
    "    return tokens"
   ],
   "id": "c0fa738e40abdd82",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:03:40.310416Z",
     "start_time": "2025-06-03T16:03:40.305688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_plag_dataset(base_path, csv_path, output_csv='similitud_todo.csv'):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df['source_dataset'].isin(['ir_plag', 'conplag'])]\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        dataset = row['source_dataset']\n",
    "        plagio = row['label']\n",
    "        file1 = row['file1']\n",
    "        file2 = row['file2']\n",
    "        file1_base = os.path.splitext(file1)[0]\n",
    "        file2_base = os.path.splitext(file2)[0]\n",
    "\n",
    "        if dataset == 'ir_plag':\n",
    "            folder = row['folder_name']\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            path1 = os.path.join(folder_path, 'original.java')\n",
    "            path2 = os.path.join(folder_path, 'compared.java')\n",
    "\n",
    "        elif dataset == 'conplag':\n",
    "            # folder_name not reliable – construct both variants\n",
    "            folder1 = os.path.join(base_path, f\"{file1_base}_{file2_base}\")\n",
    "            folder2 = os.path.join(base_path, f\"{file2_base}_{file1_base}\")\n",
    "\n",
    "            if os.path.isdir(folder1):\n",
    "                folder_path = folder1\n",
    "            elif os.path.isdir(folder2):\n",
    "                folder_path = folder2\n",
    "            else:\n",
    "                print(f\"[❌] Carpeta no encontrada para {file1} y {file2}\")\n",
    "                continue\n",
    "\n",
    "            path1 = os.path.join(folder_path, file1)\n",
    "            path2 = os.path.join(folder_path, file2)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not (os.path.exists(path1) and os.path.exists(path2)):\n",
    "            print(f\"[⚠️] Archivos no encontrados: {path1}, {path2}\")\n",
    "            continue\n",
    "\n",
    "        code1 = preprocess_file(path1)\n",
    "        code2 = preprocess_file(path2)\n",
    "\n",
    "        if not code1 or not code2:\n",
    "            print(f\"[⚠️] Código vacío: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform([code1, code2])\n",
    "        sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "\n",
    "        resultados.append({\n",
    "            'folder': os.path.basename(folder_path),\n",
    "            'file1': os.path.basename(path1),\n",
    "            'file2': os.path.basename(path2),\n",
    "            'similaridad': sim,\n",
    "            'es_plagio': plagio,\n",
    "            'dataset': dataset\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(resultados).to_csv(output_csv, index=False)\n",
    "    # Print head of the DataFrame for verification\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    print(df_resultados.head())\n",
    "    print(f\"\\n✅ Resultados guardados en {output_csv}\")"
   ],
   "id": "94a1ff418ca6e598",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def normalize_code_ast(code):\n",
    "    try:\n",
    "        tree = javalang.parse.parse(code)\n",
    "        # Normalizar nombres de variables a VAR1, VAR2, etc.\n",
    "        var_counter = 0\n",
    "        var_map = {}\n",
    "\n",
    "        # Recorrer el AST y normalizar\n",
    "        for path, node in tree.filter(javalang.tree.VariableDeclarator):\n",
    "            if node.name not in var_map:\n",
    "                var_map[node.name] = f\"VAR{var_counter}\"\n",
    "                var_counter += 1\n",
    "\n",
    "        # Similar para métodos, clases, etc.\n",
    "        return normalized_code\n",
    "    except:\n",
    "        return code"
   ],
   "id": "70de7278886f5b5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:03:41.792637Z",
     "start_time": "2025-06-03T16:03:40.319365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    BASE_PATH = 'data/splits/train'\n",
    "    CSV_PATH = 'data/splits/train.csv'\n",
    "    process_plag_dataset(BASE_PATH, CSV_PATH, output_csv='similitud_todo.csv')"
   ],
   "id": "f2443432eb9b8d5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              folder          file1          file2  similaridad  es_plagio  \\\n",
      "0  c57a973e_fa484fdd  c57a973e.java  fa484fdd.java     0.477170          1   \n",
      "1  44428e63_c850e422  44428e63.java  c850e422.java     0.403690          0   \n",
      "2  2ff0355e_83935617  2ff0355e.java  83935617.java     0.430788          0   \n",
      "3  0017d438_9852706b  0017d438.java  9852706b.java     0.755271          1   \n",
      "4  bdfe8110_c57a973e  bdfe8110.java  c57a973e.java     0.803951          1   \n",
      "\n",
      "   dataset  \n",
      "0  conplag  \n",
      "1  conplag  \n",
      "2  conplag  \n",
      "3  conplag  \n",
      "4  conplag  \n",
      "\n",
      "✅ Resultados guardados en similitud_todo.csv\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:43:44.343927Z",
     "start_time": "2025-06-03T16:43:39.484574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import javalang\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "class OptimizedJavaAnalyzer:\n",
    "    \"\"\"\n",
    "    Analizador optimizado que combina TF-IDF + AST de manera inteligente\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.java_keywords = {\n",
    "            'abstract', 'assert', 'boolean', 'break', 'byte', 'case', 'catch',\n",
    "            'char', 'class', 'const', 'continue', 'default', 'do', 'double',\n",
    "            'else', 'enum', 'extends', 'final', 'finally', 'float', 'for',\n",
    "            'goto', 'if', 'implements', 'import', 'instanceof', 'int',\n",
    "            'interface', 'long', 'native', 'new', 'package', 'private',\n",
    "            'protected', 'public', 'return', 'short', 'static', 'strictfp',\n",
    "            'super', 'switch', 'synchronized', 'this', 'throw', 'throws',\n",
    "            'transient', 'try', 'void', 'volatile', 'while'\n",
    "        }\n",
    "\n",
    "    def read_java_file(self, filepath: str) -> str:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error leyendo {filepath}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def clean_code(self, code: str) -> str:\n",
    "        if not code:\n",
    "            return \"\"\n",
    "\n",
    "        # Eliminar comentarios\n",
    "        code = re.sub(r'//.*?\\n', '\\n', code)\n",
    "        code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "\n",
    "        # Normalizar espacios pero mantener estructura\n",
    "        code = re.sub(r'[ \\t]+', ' ', code)\n",
    "        code = re.sub(r'\\n\\s*\\n', '\\n', code)\n",
    "\n",
    "        return code.strip()\n",
    "\n",
    "    def tokenize_code_advanced(self, code: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Tokenización avanzada con mejor normalización\"\"\"\n",
    "        if not code:\n",
    "            return \"\", False\n",
    "\n",
    "        try:\n",
    "            tokens = list(javalang.tokenizer.tokenize(code))\n",
    "            normalized_tokens = []\n",
    "\n",
    "            # Mapeos para normalización consistente\n",
    "            identifier_map = {}\n",
    "            var_counter = 1\n",
    "            method_counter = 1\n",
    "            class_counter = 1\n",
    "\n",
    "            for i, token in enumerate(tokens):\n",
    "                token_value = token.value\n",
    "                token_type = type(token).__name__\n",
    "\n",
    "                # Preservar palabras clave de Java\n",
    "                if token_value.lower() in self.java_keywords:\n",
    "                    normalized_tokens.append(token_value.lower())\n",
    "\n",
    "                # Normalizar identificadores por contexto\n",
    "                elif token_type == 'Identifier':\n",
    "                    if token_value not in identifier_map:\n",
    "                        # Determinar contexto más robusto\n",
    "                        prev_token = tokens[i-1].value if i > 0 else \"\"\n",
    "                        next_token = tokens[i+1].value if i < len(tokens)-1 else \"\"\n",
    "\n",
    "                        if prev_token in ['class', 'interface', 'enum']:\n",
    "                            identifier_map[token_value] = f'CLASS{class_counter}'\n",
    "                            class_counter += 1\n",
    "                        elif next_token == '(' or (i < len(tokens)-2 and tokens[i+1].value == ' ' and tokens[i+2].value == '('):\n",
    "                            identifier_map[token_value] = f'METHOD{method_counter}'\n",
    "                            method_counter += 1\n",
    "                        else:\n",
    "                            identifier_map[token_value] = f'VAR{var_counter}'\n",
    "                            var_counter += 1\n",
    "\n",
    "                    normalized_tokens.append(identifier_map[token_value])\n",
    "\n",
    "                # Preservar estructura importante\n",
    "                elif token_type in ['Operator', 'Separator']:\n",
    "                    # Agrupar operadores similares\n",
    "                    if token_value in ['==', '!=', '<', '>', '<=', '>=']:\n",
    "                        normalized_tokens.append('COMPARISON')\n",
    "                    elif token_value in ['+', '-', '*', '/', '%']:\n",
    "                        normalized_tokens.append('ARITHMETIC')\n",
    "                    elif token_value in ['&&', '||', '!']:\n",
    "                        normalized_tokens.append('LOGICAL')\n",
    "                    else:\n",
    "                        normalized_tokens.append(token_value)\n",
    "\n",
    "                # Normalizar literales\n",
    "                elif token_type in ['Integer', 'FloatingPoint']:\n",
    "                    normalized_tokens.append('NUMBER')\n",
    "                elif token_type in ['String', 'Character']:\n",
    "                    normalized_tokens.append('STRING')\n",
    "                elif token_type == 'Boolean':\n",
    "                    normalized_tokens.append('BOOLEAN')\n",
    "                else:\n",
    "                    normalized_tokens.append(token_value)\n",
    "\n",
    "            return ' '.join(normalized_tokens), True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error en tokenización: {e}\")\n",
    "            # Fallback simple\n",
    "            words = re.findall(r'\\b\\w+\\b|[+\\-*/=<>!&|{}()\\[\\];,.]', code)\n",
    "            return ' '.join(words), False\n",
    "\n",
    "    def extract_ast_features(self, code: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extrae características AST corregidas\"\"\"\n",
    "        features = {\n",
    "            'ast_success': False,\n",
    "            'num_classes': 0,\n",
    "            'num_methods': 0,\n",
    "            'num_fields': 0,\n",
    "            'num_statements': 0,\n",
    "            'num_loops': 0,\n",
    "            'num_conditionals': 0,\n",
    "            'num_method_calls': 0,\n",
    "            'num_assignments': 0,\n",
    "            'max_depth': 0,\n",
    "            'total_lines': len([l for l in code.split('\\n') if l.strip()]),\n",
    "            'cyclomatic_complexity': 1\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            tree = javalang.parse.parse(code)\n",
    "            features['ast_success'] = True\n",
    "            self._count_ast_nodes(tree, features, 0)\n",
    "        except Exception as e:\n",
    "            # Fallback léxico\n",
    "            features.update(self._lexical_fallback_analysis(code))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _count_ast_nodes(self, node, features: Dict, depth: int):\n",
    "        \"\"\"Cuenta nodos del AST de manera robusta\"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "\n",
    "        features['max_depth'] = max(features['max_depth'], depth)\n",
    "        node_type = type(node).__name__\n",
    "\n",
    "        # Contar nodos específicos\n",
    "        if node_type == 'ClassDeclaration':\n",
    "            features['num_classes'] += 1\n",
    "        elif node_type == 'MethodDeclaration':\n",
    "            features['num_methods'] += 1\n",
    "        elif node_type == 'FieldDeclaration':\n",
    "            features['num_fields'] += 1\n",
    "        elif node_type in ['BlockStatement', 'ExpressionStatement', 'ReturnStatement',\n",
    "                           'LocalVariableDeclaration']:\n",
    "            features['num_statements'] += 1\n",
    "        elif node_type in ['ForStatement', 'WhileStatement', 'DoStatement', 'EnhancedForStatement']:\n",
    "            features['num_loops'] += 1\n",
    "            features['cyclomatic_complexity'] += 1\n",
    "        elif node_type in ['IfStatement', 'SwitchStatement']:\n",
    "            features['num_conditionals'] += 1\n",
    "            features['cyclomatic_complexity'] += 1\n",
    "        elif node_type == 'MethodInvocation':\n",
    "            features['num_method_calls'] += 1\n",
    "        elif node_type == 'Assignment':\n",
    "            features['num_assignments'] += 1\n",
    "\n",
    "        # Recursión\n",
    "        if hasattr(node, 'children'):\n",
    "            for child in node.children:\n",
    "                if child is not None:\n",
    "                    if isinstance(child, list):\n",
    "                        for subchild in child:\n",
    "                            if subchild is not None:\n",
    "                                self._count_ast_nodes(subchild, features, depth + 1)\n",
    "                    else:\n",
    "                        self._count_ast_nodes(child, features, depth + 1)\n",
    "\n",
    "    def _lexical_fallback_analysis(self, code: str) -> Dict[str, int]:\n",
    "        \"\"\"Análisis léxico cuando AST falla\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        features['num_classes'] = len(re.findall(r'\\bclass\\s+\\w+', code))\n",
    "        features['num_methods'] = len(re.findall(r'\\b\\w+\\s*\\([^)]*\\)\\s*\\{', code))\n",
    "        features['num_loops'] = (code.count('for(') + code.count('for (') +\n",
    "                                 code.count('while(') + code.count('while ('))\n",
    "        features['num_conditionals'] = code.count('if(') + code.count('if (')\n",
    "        features['num_method_calls'] = len(re.findall(r'\\w+\\s*\\(', code)) - features['num_methods']\n",
    "        features['num_assignments'] = code.count('=') - code.count('==') - code.count('!=')\n",
    "        features['cyclomatic_complexity'] = 1 + features['num_loops'] + features['num_conditionals']\n",
    "\n",
    "        # Profundidad por llaves\n",
    "        max_braces = 0\n",
    "        current_braces = 0\n",
    "        for char in code:\n",
    "            if char == '{':\n",
    "                current_braces += 1\n",
    "                max_braces = max(max_braces, current_braces)\n",
    "            elif char == '}':\n",
    "                current_braces -= 1\n",
    "        features['max_depth'] = max_braces\n",
    "\n",
    "        return features\n",
    "\n",
    "    def compute_intelligent_ast_similarity(self, features1: Dict, features2: Dict,\n",
    "                                           dataset_type: str) -> float:\n",
    "        \"\"\"\n",
    "        Similitud AST inteligente que considera el tipo de dataset\n",
    "        \"\"\"\n",
    "        structural_features = [\n",
    "            'num_classes', 'num_methods', 'num_fields', 'num_statements',\n",
    "            'num_loops', 'num_conditionals', 'num_method_calls',\n",
    "            'num_assignments', 'max_depth', 'cyclomatic_complexity'\n",
    "        ]\n",
    "\n",
    "        similarities = []\n",
    "\n",
    "        for feature in structural_features:\n",
    "            val1 = features1.get(feature, 0)\n",
    "            val2 = features2.get(feature, 0)\n",
    "\n",
    "            if val1 == 0 and val2 == 0:\n",
    "                similarity = 1.0\n",
    "            else:\n",
    "                max_val = max(val1, val2)\n",
    "                min_val = min(val1, val2)\n",
    "\n",
    "                if max_val == 0:\n",
    "                    similarity = 1.0\n",
    "                else:\n",
    "                    similarity = min_val / max_val\n",
    "\n",
    "            similarities.append(similarity)\n",
    "\n",
    "        # Pesos adaptativos según el dataset\n",
    "        if dataset_type == 'conplag':\n",
    "            # Para ConPlag, dar menos peso a similitud estructural\n",
    "            # (códigos similares por resolver mismo problema)\n",
    "            weights = [0.05, 0.10, 0.05, 0.15, 0.15, 0.15, 0.10, 0.10, 0.10, 0.05]\n",
    "        else:  # ir_plag\n",
    "            # Para IR-Plag, la estructura puede ser más discriminativa\n",
    "            weights = [0.15, 0.20, 0.10, 0.15, 0.10, 0.10, 0.05, 0.05, 0.05, 0.05]\n",
    "\n",
    "        weighted_similarity = np.average(similarities, weights=weights)\n",
    "        return weighted_similarity\n",
    "\n",
    "def enhanced_tfidf_similarity(code1: str, code2: str,\n",
    "                              dataset_type: str = 'general') -> float:\n",
    "    \"\"\"TF-IDF optimizado con configuración adaptativa\"\"\"\n",
    "    if not code1 or not code2:\n",
    "        return 0.0\n",
    "\n",
    "    # Configuración adaptativa\n",
    "    if dataset_type == 'conplag':\n",
    "        # Para ConPlag: más n-grams para capturar patrones de código\n",
    "        ngram_range = (1, 3)\n",
    "        max_features = 4000\n",
    "    else:  # ir_plag\n",
    "        # Para IR-Plag: configuración balanceada\n",
    "        ngram_range = (1, 2)\n",
    "        max_features = 3000\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        max_features=max_features,\n",
    "        min_df=1,\n",
    "        max_df=1.0,\n",
    "        lowercase=False,\n",
    "        token_pattern=r'\\b\\w+\\b'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([code1, code2])\n",
    "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        return similarity\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def process_dataset_optimized(base_path: str, csv_path: str,\n",
    "                              output_csv: str = 'similitud_optimized.csv'):\n",
    "    \"\"\"Procesa dataset con análisis optimizado\"\"\"\n",
    "    print(\"🚀 PROCESAMIENTO OPTIMIZADO DEL DATASET\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    analyzer = OptimizedJavaAnalyzer()\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df['source_dataset'].isin(['ir_plag', 'conplag'])]\n",
    "\n",
    "    print(f\"📊 Procesando {len(df)} pares...\")\n",
    "\n",
    "    resultados = []\n",
    "    processed = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        dataset = row['source_dataset']\n",
    "        plagio = row['label']\n",
    "        file1 = row['file1']\n",
    "        file2 = row['file2']\n",
    "        file1_base = os.path.splitext(file1)[0]\n",
    "        file2_base = os.path.splitext(file2)[0]\n",
    "\n",
    "        # Determinar rutas\n",
    "        if dataset == 'ir_plag':\n",
    "            folder = row['folder_name']\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            path1 = os.path.join(folder_path, 'original.java')\n",
    "            path2 = os.path.join(folder_path, 'compared.java')\n",
    "        else:  # conplag\n",
    "            folder1 = os.path.join(base_path, f\"{file1_base}_{file2_base}\")\n",
    "            folder2 = os.path.join(base_path, f\"{file2_base}_{file1_base}\")\n",
    "\n",
    "            if os.path.isdir(folder1):\n",
    "                folder_path = folder1\n",
    "            elif os.path.isdir(folder2):\n",
    "                folder_path = folder2\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            path1 = os.path.join(folder_path, file1)\n",
    "            path2 = os.path.join(folder_path, file2)\n",
    "\n",
    "        if not (os.path.exists(path1) and os.path.exists(path2)):\n",
    "            continue\n",
    "\n",
    "        # Procesar archivos\n",
    "        raw_code1 = analyzer.read_java_file(path1)\n",
    "        raw_code2 = analyzer.read_java_file(path2)\n",
    "\n",
    "        cleaned_code1 = analyzer.clean_code(raw_code1)\n",
    "        cleaned_code2 = analyzer.clean_code(raw_code2)\n",
    "\n",
    "        tokens1, success1 = analyzer.tokenize_code_advanced(cleaned_code1)\n",
    "        tokens2, success2 = analyzer.tokenize_code_advanced(cleaned_code2)\n",
    "\n",
    "        if not tokens1 or not tokens2:\n",
    "            continue\n",
    "\n",
    "        # Calcular similitudes\n",
    "        tfidf_sim = enhanced_tfidf_similarity(tokens1, tokens2, dataset)\n",
    "\n",
    "        ast_features1 = analyzer.extract_ast_features(cleaned_code1)\n",
    "        ast_features2 = analyzer.extract_ast_features(cleaned_code2)\n",
    "        ast_sim = analyzer.compute_intelligent_ast_similarity(\n",
    "            ast_features1, ast_features2, dataset\n",
    "        )\n",
    "\n",
    "        # Similitud combinada adaptativa\n",
    "        if dataset == 'conplag':\n",
    "            # Para ConPlag: más peso a TF-IDF\n",
    "            combined_sim = 0.85 * tfidf_sim + 0.15 * ast_sim\n",
    "        else:  # ir_plag\n",
    "            # Para IR-Plag: balance 70-30\n",
    "            combined_sim = 0.70 * tfidf_sim + 0.30 * ast_sim\n",
    "\n",
    "        # Características adicionales para ML\n",
    "        length_ratio = min(len(tokens1.split()), len(tokens2.split())) / max(len(tokens1.split()), len(tokens2.split()), 1)\n",
    "        complexity_ratio = min(ast_features1.get('cyclomatic_complexity', 1), ast_features2.get('cyclomatic_complexity', 1)) / max(ast_features1.get('cyclomatic_complexity', 1), ast_features2.get('cyclomatic_complexity', 1))\n",
    "\n",
    "        resultado = {\n",
    "            'folder': os.path.basename(folder_path),\n",
    "            'file1': os.path.basename(path1),\n",
    "            'file2': os.path.basename(path2),\n",
    "            'tfidf_similarity': round(tfidf_sim, 4),\n",
    "            'ast_similarity': round(ast_sim, 4),\n",
    "            'combined_similarity': round(combined_sim, 4),\n",
    "            'length_ratio': round(length_ratio, 4),\n",
    "            'complexity_ratio': round(complexity_ratio, 4),\n",
    "            'es_plagio': plagio,\n",
    "            'dataset': dataset,\n",
    "            'tokenize_success': success1 and success2,\n",
    "            'ast_success': ast_features1['ast_success'] and ast_features2['ast_success']\n",
    "        }\n",
    "\n",
    "        resultados.append(resultado)\n",
    "        processed += 1\n",
    "\n",
    "        if processed % 100 == 0:\n",
    "            print(f\"✅ Procesados {processed}/{len(df)} pares...\")\n",
    "\n",
    "    # Guardar y analizar resultados\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    df_resultados.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"\\n📊 RESULTADOS OPTIMIZADOS:\")\n",
    "    print(f\"✅ Pares procesados: {len(df_resultados)}\")\n",
    "\n",
    "    # Análisis por dataset\n",
    "    for dataset in ['ir_plag', 'conplag']:\n",
    "        subset = df_resultados[df_resultados['dataset'] == dataset]\n",
    "        if len(subset) > 0:\n",
    "            plagiados = subset[subset['es_plagio'] == 1]\n",
    "            no_plagiados = subset[subset['es_plagio'] == 0]\n",
    "\n",
    "            print(f\"\\n📈 {dataset.upper()}:\")\n",
    "            if len(plagiados) > 0:\n",
    "                print(f\"  Plagiados - TF-IDF: {plagiados['tfidf_similarity'].mean():.3f}\")\n",
    "                print(f\"  Plagiados - AST: {plagiados['ast_similarity'].mean():.3f}\")\n",
    "                print(f\"  Plagiados - Combinado: {plagiados['combined_similarity'].mean():.3f}\")\n",
    "            if len(no_plagiados) > 0:\n",
    "                print(f\"  No plagiados - TF-IDF: {no_plagiados['tfidf_similarity'].mean():.3f}\")\n",
    "                print(f\"  No plagiados - AST: {no_plagiados['ast_similarity'].mean():.3f}\")\n",
    "                print(f\"  No plagiados - Combinado: {no_plagiados['combined_similarity'].mean():.3f}\")\n",
    "\n",
    "    print(f\"\\n💾 Resultados guardados en: {output_csv}\")\n",
    "    print(df_resultados.head())\n",
    "\n",
    "    return df_resultados\n",
    "\n",
    "def evaluate_thresholds(df_results: pd.DataFrame):\n",
    "    \"\"\"Evalúa diferentes thresholds para encontrar el óptimo\"\"\"\n",
    "    print(f\"\\n🎯 EVALUACIÓN DE THRESHOLDS\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    features = ['tfidf_similarity', 'ast_similarity', 'combined_similarity']\n",
    "    thresholds = np.arange(0.3, 0.8, 0.05)\n",
    "\n",
    "    best_results = {}\n",
    "\n",
    "    for feature in features:\n",
    "        best_acc = 0\n",
    "        best_thresh = 0\n",
    "\n",
    "        print(f\"\\n📊 {feature.upper()}:\")\n",
    "        for thresh in thresholds:\n",
    "            predictions = (df_results[feature] > thresh).astype(int)\n",
    "            accuracy = (predictions == df_results['es_plagio']).mean()\n",
    "\n",
    "            if accuracy > best_acc:\n",
    "                best_acc = accuracy\n",
    "                best_thresh = thresh\n",
    "\n",
    "            print(f\"  Threshold {thresh:.2f}: Accuracy {accuracy:.3f}\")\n",
    "\n",
    "        best_results[feature] = {'threshold': best_thresh, 'accuracy': best_acc}\n",
    "        print(f\"  🏆 Mejor: {best_thresh:.2f} (Accuracy: {best_acc:.3f})\")\n",
    "\n",
    "    return best_results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BASE_PATH = 'data/splits/train'\n",
    "    CSV_PATH = 'data/splits/train.csv'\n",
    "\n",
    "    # Procesamiento optimizado\n",
    "    df_results = process_dataset_optimized(BASE_PATH, CSV_PATH)\n",
    "\n",
    "    # Evaluación de thresholds\n",
    "    best_thresholds = evaluate_thresholds(df_results)"
   ],
   "id": "85807374b2ff9c9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 PROCESAMIENTO OPTIMIZADO DEL DATASET\n",
      "=============================================\n",
      "📊 Procesando 957 pares...\n",
      "✅ Procesados 100/957 pares...\n",
      "✅ Procesados 200/957 pares...\n",
      "✅ Procesados 300/957 pares...\n",
      "✅ Procesados 400/957 pares...\n",
      "✅ Procesados 500/957 pares...\n",
      "✅ Procesados 600/957 pares...\n",
      "✅ Procesados 700/957 pares...\n",
      "✅ Procesados 800/957 pares...\n",
      "✅ Procesados 900/957 pares...\n",
      "\n",
      "📊 RESULTADOS OPTIMIZADOS:\n",
      "✅ Pares procesados: 957\n",
      "\n",
      "📈 IR_PLAG:\n",
      "  Plagiados - TF-IDF: 0.678\n",
      "  Plagiados - AST: 0.866\n",
      "  Plagiados - Combinado: 0.734\n",
      "  No plagiados - TF-IDF: 0.667\n",
      "  No plagiados - AST: 0.885\n",
      "  No plagiados - Combinado: 0.732\n",
      "\n",
      "📈 CONPLAG:\n",
      "  Plagiados - TF-IDF: 0.584\n",
      "  Plagiados - AST: 0.894\n",
      "  Plagiados - Combinado: 0.631\n",
      "  No plagiados - TF-IDF: 0.423\n",
      "  No plagiados - AST: 0.722\n",
      "  No plagiados - Combinado: 0.468\n",
      "\n",
      "💾 Resultados guardados en: similitud_optimized.csv\n",
      "              folder          file1          file2  tfidf_similarity  \\\n",
      "0  c57a973e_fa484fdd  c57a973e.java  fa484fdd.java            0.3820   \n",
      "1  44428e63_c850e422  44428e63.java  c850e422.java            0.4771   \n",
      "2  2ff0355e_83935617  2ff0355e.java  83935617.java            0.4478   \n",
      "3  0017d438_9852706b  0017d438.java  9852706b.java            0.4633   \n",
      "4  bdfe8110_c57a973e  bdfe8110.java  c57a973e.java            0.3879   \n",
      "\n",
      "   ast_similarity  combined_similarity  length_ratio  complexity_ratio  \\\n",
      "0          0.7127               0.4316        0.9347            0.7500   \n",
      "1          0.8162               0.5280        0.6466            1.0000   \n",
      "2          0.5591               0.4645        0.6274            0.5625   \n",
      "3          0.9104               0.5303        0.8854            1.0000   \n",
      "4          0.8741               0.4608        0.9205            0.7500   \n",
      "\n",
      "   es_plagio  dataset  tokenize_success  ast_success  \n",
      "0          1  conplag              True         True  \n",
      "1          0  conplag              True         True  \n",
      "2          0  conplag              True         True  \n",
      "3          1  conplag              True         True  \n",
      "4          1  conplag              True         True  \n",
      "\n",
      "🎯 EVALUACIÓN DE THRESHOLDS\n",
      "------------------------------\n",
      "\n",
      "📊 TFIDF_SIMILARITY:\n",
      "  Threshold 0.30: Accuracy 0.462\n",
      "  Threshold 0.35: Accuracy 0.502\n",
      "  Threshold 0.40: Accuracy 0.575\n",
      "  Threshold 0.45: Accuracy 0.658\n",
      "  Threshold 0.50: Accuracy 0.717\n",
      "  Threshold 0.55: Accuracy 0.708\n",
      "  Threshold 0.60: Accuracy 0.689\n",
      "  Threshold 0.65: Accuracy 0.675\n",
      "  Threshold 0.70: Accuracy 0.673\n",
      "  Threshold 0.75: Accuracy 0.661\n",
      "  🏆 Mejor: 0.50 (Accuracy: 0.717)\n",
      "\n",
      "📊 AST_SIMILARITY:\n",
      "  Threshold 0.30: Accuracy 0.442\n",
      "  Threshold 0.35: Accuracy 0.442\n",
      "  Threshold 0.40: Accuracy 0.442\n",
      "  Threshold 0.45: Accuracy 0.444\n",
      "  Threshold 0.50: Accuracy 0.448\n",
      "  Threshold 0.55: Accuracy 0.466\n",
      "  Threshold 0.60: Accuracy 0.485\n",
      "  Threshold 0.65: Accuracy 0.541\n",
      "  Threshold 0.70: Accuracy 0.618\n",
      "  Threshold 0.75: Accuracy 0.682\n",
      "  🏆 Mejor: 0.75 (Accuracy: 0.682)\n",
      "\n",
      "📊 COMBINED_SIMILARITY:\n",
      "  Threshold 0.30: Accuracy 0.447\n",
      "  Threshold 0.35: Accuracy 0.462\n",
      "  Threshold 0.40: Accuracy 0.503\n",
      "  Threshold 0.45: Accuracy 0.601\n",
      "  Threshold 0.50: Accuracy 0.690\n",
      "  Threshold 0.55: Accuracy 0.755\n",
      "  Threshold 0.60: Accuracy 0.746\n",
      "  Threshold 0.65: Accuracy 0.703\n",
      "  Threshold 0.70: Accuracy 0.692\n",
      "  Threshold 0.75: Accuracy 0.674\n",
      "  🏆 Mejor: 0.55 (Accuracy: 0.755)\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
