{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:03:40.284689Z",
     "start_time": "2025-06-03T16:03:40.281758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import javalang\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:03:40.297891Z",
     "start_time": "2025-06-03T16:03:40.293860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_java_file(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        return f.read()\n",
    "\n",
    "def clean_code(code):\n",
    "    import re\n",
    "    code = re.sub(r'//.*?\\n', '\\n', code)\n",
    "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "    code = re.sub(r'\\s+', ' ', code).strip()\n",
    "    return code\n",
    "\n",
    "def tokenize_code(code):\n",
    "    try:\n",
    "        tokens = list(javalang.tokenizer.tokenize(code))\n",
    "        return ' '.join(token.value for token in tokens)\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "def preprocess_file(filepath):\n",
    "    raw = read_java_file(filepath)\n",
    "    cleaned = clean_code(raw)\n",
    "    tokens = tokenize_code(cleaned)\n",
    "    return tokens"
   ],
   "id": "c0fa738e40abdd82",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:03:40.310416Z",
     "start_time": "2025-06-03T16:03:40.305688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_plag_dataset(base_path, csv_path, output_csv='similitud_todo.csv'):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df['source_dataset'].isin(['ir_plag', 'conplag'])]\n",
    "\n",
    "    resultados = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        dataset = row['source_dataset']\n",
    "        plagio = row['label']\n",
    "        file1 = row['file1']\n",
    "        file2 = row['file2']\n",
    "        file1_base = os.path.splitext(file1)[0]\n",
    "        file2_base = os.path.splitext(file2)[0]\n",
    "\n",
    "        if dataset == 'ir_plag':\n",
    "            folder = row['folder_name']\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            path1 = os.path.join(folder_path, 'original.java')\n",
    "            path2 = os.path.join(folder_path, 'compared.java')\n",
    "\n",
    "        elif dataset == 'conplag':\n",
    "            # folder_name not reliable ‚Äì construct both variants\n",
    "            folder1 = os.path.join(base_path, f\"{file1_base}_{file2_base}\")\n",
    "            folder2 = os.path.join(base_path, f\"{file2_base}_{file1_base}\")\n",
    "\n",
    "            if os.path.isdir(folder1):\n",
    "                folder_path = folder1\n",
    "            elif os.path.isdir(folder2):\n",
    "                folder_path = folder2\n",
    "            else:\n",
    "                print(f\"[‚ùå] Carpeta no encontrada para {file1} y {file2}\")\n",
    "                continue\n",
    "\n",
    "            path1 = os.path.join(folder_path, file1)\n",
    "            path2 = os.path.join(folder_path, file2)\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if not (os.path.exists(path1) and os.path.exists(path2)):\n",
    "            print(f\"[‚ö†Ô∏è] Archivos no encontrados: {path1}, {path2}\")\n",
    "            continue\n",
    "\n",
    "        code1 = preprocess_file(path1)\n",
    "        code2 = preprocess_file(path2)\n",
    "\n",
    "        if not code1 or not code2:\n",
    "            print(f\"[‚ö†Ô∏è] C√≥digo vac√≠o: {folder_path}\")\n",
    "            continue\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        tfidf_matrix = vectorizer.fit_transform([code1, code2])\n",
    "        sim = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]\n",
    "\n",
    "        resultados.append({\n",
    "            'folder': os.path.basename(folder_path),\n",
    "            'file1': os.path.basename(path1),\n",
    "            'file2': os.path.basename(path2),\n",
    "            'similaridad': sim,\n",
    "            'es_plagio': plagio,\n",
    "            'dataset': dataset\n",
    "        })\n",
    "\n",
    "    pd.DataFrame(resultados).to_csv(output_csv, index=False)\n",
    "    # Print head of the DataFrame for verification\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    print(df_resultados.head())\n",
    "    print(f\"\\n‚úÖ Resultados guardados en {output_csv}\")"
   ],
   "id": "94a1ff418ca6e598",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def normalize_code_ast(code):\n",
    "    try:\n",
    "        tree = javalang.parse.parse(code)\n",
    "        # Normalizar nombres de variables a VAR1, VAR2, etc.\n",
    "        var_counter = 0\n",
    "        var_map = {}\n",
    "\n",
    "        # Recorrer el AST y normalizar\n",
    "        for path, node in tree.filter(javalang.tree.VariableDeclarator):\n",
    "            if node.name not in var_map:\n",
    "                var_map[node.name] = f\"VAR{var_counter}\"\n",
    "                var_counter += 1\n",
    "\n",
    "        # Similar para m√©todos, clases, etc.\n",
    "        return normalized_code\n",
    "    except:\n",
    "        return code"
   ],
   "id": "70de7278886f5b5e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:03:41.792637Z",
     "start_time": "2025-06-03T16:03:40.319365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == '__main__':\n",
    "    BASE_PATH = 'data/splits/train'\n",
    "    CSV_PATH = 'data/splits/train.csv'\n",
    "    process_plag_dataset(BASE_PATH, CSV_PATH, output_csv='similitud_todo.csv')"
   ],
   "id": "f2443432eb9b8d5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              folder          file1          file2  similaridad  es_plagio  \\\n",
      "0  c57a973e_fa484fdd  c57a973e.java  fa484fdd.java     0.477170          1   \n",
      "1  44428e63_c850e422  44428e63.java  c850e422.java     0.403690          0   \n",
      "2  2ff0355e_83935617  2ff0355e.java  83935617.java     0.430788          0   \n",
      "3  0017d438_9852706b  0017d438.java  9852706b.java     0.755271          1   \n",
      "4  bdfe8110_c57a973e  bdfe8110.java  c57a973e.java     0.803951          1   \n",
      "\n",
      "   dataset  \n",
      "0  conplag  \n",
      "1  conplag  \n",
      "2  conplag  \n",
      "3  conplag  \n",
      "4  conplag  \n",
      "\n",
      "‚úÖ Resultados guardados en similitud_todo.csv\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-03T16:43:44.343927Z",
     "start_time": "2025-06-03T16:43:39.484574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import javalang\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "class OptimizedJavaAnalyzer:\n",
    "    \"\"\"\n",
    "    Analizador optimizado que combina TF-IDF + AST de manera inteligente\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.java_keywords = {\n",
    "            'abstract', 'assert', 'boolean', 'break', 'byte', 'case', 'catch',\n",
    "            'char', 'class', 'const', 'continue', 'default', 'do', 'double',\n",
    "            'else', 'enum', 'extends', 'final', 'finally', 'float', 'for',\n",
    "            'goto', 'if', 'implements', 'import', 'instanceof', 'int',\n",
    "            'interface', 'long', 'native', 'new', 'package', 'private',\n",
    "            'protected', 'public', 'return', 'short', 'static', 'strictfp',\n",
    "            'super', 'switch', 'synchronized', 'this', 'throw', 'throws',\n",
    "            'transient', 'try', 'void', 'volatile', 'while'\n",
    "        }\n",
    "\n",
    "    def read_java_file(self, filepath: str) -> str:\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                return f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error leyendo {filepath}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def clean_code(self, code: str) -> str:\n",
    "        if not code:\n",
    "            return \"\"\n",
    "\n",
    "        # Eliminar comentarios\n",
    "        code = re.sub(r'//.*?\\n', '\\n', code)\n",
    "        code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
    "\n",
    "        # Normalizar espacios pero mantener estructura\n",
    "        code = re.sub(r'[ \\t]+', ' ', code)\n",
    "        code = re.sub(r'\\n\\s*\\n', '\\n', code)\n",
    "\n",
    "        return code.strip()\n",
    "\n",
    "    def tokenize_code_advanced(self, code: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Tokenizaci√≥n avanzada con mejor normalizaci√≥n\"\"\"\n",
    "        if not code:\n",
    "            return \"\", False\n",
    "\n",
    "        try:\n",
    "            tokens = list(javalang.tokenizer.tokenize(code))\n",
    "            normalized_tokens = []\n",
    "\n",
    "            # Mapeos para normalizaci√≥n consistente\n",
    "            identifier_map = {}\n",
    "            var_counter = 1\n",
    "            method_counter = 1\n",
    "            class_counter = 1\n",
    "\n",
    "            for i, token in enumerate(tokens):\n",
    "                token_value = token.value\n",
    "                token_type = type(token).__name__\n",
    "\n",
    "                # Preservar palabras clave de Java\n",
    "                if token_value.lower() in self.java_keywords:\n",
    "                    normalized_tokens.append(token_value.lower())\n",
    "\n",
    "                # Normalizar identificadores por contexto\n",
    "                elif token_type == 'Identifier':\n",
    "                    if token_value not in identifier_map:\n",
    "                        # Determinar contexto m√°s robusto\n",
    "                        prev_token = tokens[i-1].value if i > 0 else \"\"\n",
    "                        next_token = tokens[i+1].value if i < len(tokens)-1 else \"\"\n",
    "\n",
    "                        if prev_token in ['class', 'interface', 'enum']:\n",
    "                            identifier_map[token_value] = f'CLASS{class_counter}'\n",
    "                            class_counter += 1\n",
    "                        elif next_token == '(' or (i < len(tokens)-2 and tokens[i+1].value == ' ' and tokens[i+2].value == '('):\n",
    "                            identifier_map[token_value] = f'METHOD{method_counter}'\n",
    "                            method_counter += 1\n",
    "                        else:\n",
    "                            identifier_map[token_value] = f'VAR{var_counter}'\n",
    "                            var_counter += 1\n",
    "\n",
    "                    normalized_tokens.append(identifier_map[token_value])\n",
    "\n",
    "                # Preservar estructura importante\n",
    "                elif token_type in ['Operator', 'Separator']:\n",
    "                    # Agrupar operadores similares\n",
    "                    if token_value in ['==', '!=', '<', '>', '<=', '>=']:\n",
    "                        normalized_tokens.append('COMPARISON')\n",
    "                    elif token_value in ['+', '-', '*', '/', '%']:\n",
    "                        normalized_tokens.append('ARITHMETIC')\n",
    "                    elif token_value in ['&&', '||', '!']:\n",
    "                        normalized_tokens.append('LOGICAL')\n",
    "                    else:\n",
    "                        normalized_tokens.append(token_value)\n",
    "\n",
    "                # Normalizar literales\n",
    "                elif token_type in ['Integer', 'FloatingPoint']:\n",
    "                    normalized_tokens.append('NUMBER')\n",
    "                elif token_type in ['String', 'Character']:\n",
    "                    normalized_tokens.append('STRING')\n",
    "                elif token_type == 'Boolean':\n",
    "                    normalized_tokens.append('BOOLEAN')\n",
    "                else:\n",
    "                    normalized_tokens.append(token_value)\n",
    "\n",
    "            return ' '.join(normalized_tokens), True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error en tokenizaci√≥n: {e}\")\n",
    "            # Fallback simple\n",
    "            words = re.findall(r'\\b\\w+\\b|[+\\-*/=<>!&|{}()\\[\\];,.]', code)\n",
    "            return ' '.join(words), False\n",
    "\n",
    "    def extract_ast_features(self, code: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extrae caracter√≠sticas AST corregidas\"\"\"\n",
    "        features = {\n",
    "            'ast_success': False,\n",
    "            'num_classes': 0,\n",
    "            'num_methods': 0,\n",
    "            'num_fields': 0,\n",
    "            'num_statements': 0,\n",
    "            'num_loops': 0,\n",
    "            'num_conditionals': 0,\n",
    "            'num_method_calls': 0,\n",
    "            'num_assignments': 0,\n",
    "            'max_depth': 0,\n",
    "            'total_lines': len([l for l in code.split('\\n') if l.strip()]),\n",
    "            'cyclomatic_complexity': 1\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            tree = javalang.parse.parse(code)\n",
    "            features['ast_success'] = True\n",
    "            self._count_ast_nodes(tree, features, 0)\n",
    "        except Exception as e:\n",
    "            # Fallback l√©xico\n",
    "            features.update(self._lexical_fallback_analysis(code))\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _count_ast_nodes(self, node, features: Dict, depth: int):\n",
    "        \"\"\"Cuenta nodos del AST de manera robusta\"\"\"\n",
    "        if node is None:\n",
    "            return\n",
    "\n",
    "        features['max_depth'] = max(features['max_depth'], depth)\n",
    "        node_type = type(node).__name__\n",
    "\n",
    "        # Contar nodos espec√≠ficos\n",
    "        if node_type == 'ClassDeclaration':\n",
    "            features['num_classes'] += 1\n",
    "        elif node_type == 'MethodDeclaration':\n",
    "            features['num_methods'] += 1\n",
    "        elif node_type == 'FieldDeclaration':\n",
    "            features['num_fields'] += 1\n",
    "        elif node_type in ['BlockStatement', 'ExpressionStatement', 'ReturnStatement',\n",
    "                           'LocalVariableDeclaration']:\n",
    "            features['num_statements'] += 1\n",
    "        elif node_type in ['ForStatement', 'WhileStatement', 'DoStatement', 'EnhancedForStatement']:\n",
    "            features['num_loops'] += 1\n",
    "            features['cyclomatic_complexity'] += 1\n",
    "        elif node_type in ['IfStatement', 'SwitchStatement']:\n",
    "            features['num_conditionals'] += 1\n",
    "            features['cyclomatic_complexity'] += 1\n",
    "        elif node_type == 'MethodInvocation':\n",
    "            features['num_method_calls'] += 1\n",
    "        elif node_type == 'Assignment':\n",
    "            features['num_assignments'] += 1\n",
    "\n",
    "        # Recursi√≥n\n",
    "        if hasattr(node, 'children'):\n",
    "            for child in node.children:\n",
    "                if child is not None:\n",
    "                    if isinstance(child, list):\n",
    "                        for subchild in child:\n",
    "                            if subchild is not None:\n",
    "                                self._count_ast_nodes(subchild, features, depth + 1)\n",
    "                    else:\n",
    "                        self._count_ast_nodes(child, features, depth + 1)\n",
    "\n",
    "    def _lexical_fallback_analysis(self, code: str) -> Dict[str, int]:\n",
    "        \"\"\"An√°lisis l√©xico cuando AST falla\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        features['num_classes'] = len(re.findall(r'\\bclass\\s+\\w+', code))\n",
    "        features['num_methods'] = len(re.findall(r'\\b\\w+\\s*\\([^)]*\\)\\s*\\{', code))\n",
    "        features['num_loops'] = (code.count('for(') + code.count('for (') +\n",
    "                                 code.count('while(') + code.count('while ('))\n",
    "        features['num_conditionals'] = code.count('if(') + code.count('if (')\n",
    "        features['num_method_calls'] = len(re.findall(r'\\w+\\s*\\(', code)) - features['num_methods']\n",
    "        features['num_assignments'] = code.count('=') - code.count('==') - code.count('!=')\n",
    "        features['cyclomatic_complexity'] = 1 + features['num_loops'] + features['num_conditionals']\n",
    "\n",
    "        # Profundidad por llaves\n",
    "        max_braces = 0\n",
    "        current_braces = 0\n",
    "        for char in code:\n",
    "            if char == '{':\n",
    "                current_braces += 1\n",
    "                max_braces = max(max_braces, current_braces)\n",
    "            elif char == '}':\n",
    "                current_braces -= 1\n",
    "        features['max_depth'] = max_braces\n",
    "\n",
    "        return features\n",
    "\n",
    "    def compute_intelligent_ast_similarity(self, features1: Dict, features2: Dict,\n",
    "                                           dataset_type: str) -> float:\n",
    "        \"\"\"\n",
    "        Similitud AST inteligente que considera el tipo de dataset\n",
    "        \"\"\"\n",
    "        structural_features = [\n",
    "            'num_classes', 'num_methods', 'num_fields', 'num_statements',\n",
    "            'num_loops', 'num_conditionals', 'num_method_calls',\n",
    "            'num_assignments', 'max_depth', 'cyclomatic_complexity'\n",
    "        ]\n",
    "\n",
    "        similarities = []\n",
    "\n",
    "        for feature in structural_features:\n",
    "            val1 = features1.get(feature, 0)\n",
    "            val2 = features2.get(feature, 0)\n",
    "\n",
    "            if val1 == 0 and val2 == 0:\n",
    "                similarity = 1.0\n",
    "            else:\n",
    "                max_val = max(val1, val2)\n",
    "                min_val = min(val1, val2)\n",
    "\n",
    "                if max_val == 0:\n",
    "                    similarity = 1.0\n",
    "                else:\n",
    "                    similarity = min_val / max_val\n",
    "\n",
    "            similarities.append(similarity)\n",
    "\n",
    "        # Pesos adaptativos seg√∫n el dataset\n",
    "        if dataset_type == 'conplag':\n",
    "            # Para ConPlag, dar menos peso a similitud estructural\n",
    "            # (c√≥digos similares por resolver mismo problema)\n",
    "            weights = [0.05, 0.10, 0.05, 0.15, 0.15, 0.15, 0.10, 0.10, 0.10, 0.05]\n",
    "        else:  # ir_plag\n",
    "            # Para IR-Plag, la estructura puede ser m√°s discriminativa\n",
    "            weights = [0.15, 0.20, 0.10, 0.15, 0.10, 0.10, 0.05, 0.05, 0.05, 0.05]\n",
    "\n",
    "        weighted_similarity = np.average(similarities, weights=weights)\n",
    "        return weighted_similarity\n",
    "\n",
    "def enhanced_tfidf_similarity(code1: str, code2: str,\n",
    "                              dataset_type: str = 'general') -> float:\n",
    "    \"\"\"TF-IDF optimizado con configuraci√≥n adaptativa\"\"\"\n",
    "    if not code1 or not code2:\n",
    "        return 0.0\n",
    "\n",
    "    # Configuraci√≥n adaptativa\n",
    "    if dataset_type == 'conplag':\n",
    "        # Para ConPlag: m√°s n-grams para capturar patrones de c√≥digo\n",
    "        ngram_range = (1, 3)\n",
    "        max_features = 4000\n",
    "    else:  # ir_plag\n",
    "        # Para IR-Plag: configuraci√≥n balanceada\n",
    "        ngram_range = (1, 2)\n",
    "        max_features = 3000\n",
    "\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        max_features=max_features,\n",
    "        min_df=1,\n",
    "        max_df=1.0,\n",
    "        lowercase=False,\n",
    "        token_pattern=r'\\b\\w+\\b'\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform([code1, code2])\n",
    "        similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "        return similarity\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "def process_dataset_optimized(base_path: str, csv_path: str,\n",
    "                              output_csv: str = 'similitud_optimized.csv'):\n",
    "    \"\"\"Procesa dataset con an√°lisis optimizado\"\"\"\n",
    "    print(\"üöÄ PROCESAMIENTO OPTIMIZADO DEL DATASET\")\n",
    "    print(\"=\" * 45)\n",
    "\n",
    "    analyzer = OptimizedJavaAnalyzer()\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df['source_dataset'].isin(['ir_plag', 'conplag'])]\n",
    "\n",
    "    print(f\"üìä Procesando {len(df)} pares...\")\n",
    "\n",
    "    resultados = []\n",
    "    processed = 0\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        dataset = row['source_dataset']\n",
    "        plagio = row['label']\n",
    "        file1 = row['file1']\n",
    "        file2 = row['file2']\n",
    "        file1_base = os.path.splitext(file1)[0]\n",
    "        file2_base = os.path.splitext(file2)[0]\n",
    "\n",
    "        # Determinar rutas\n",
    "        if dataset == 'ir_plag':\n",
    "            folder = row['folder_name']\n",
    "            folder_path = os.path.join(base_path, folder)\n",
    "            path1 = os.path.join(folder_path, 'original.java')\n",
    "            path2 = os.path.join(folder_path, 'compared.java')\n",
    "        else:  # conplag\n",
    "            folder1 = os.path.join(base_path, f\"{file1_base}_{file2_base}\")\n",
    "            folder2 = os.path.join(base_path, f\"{file2_base}_{file1_base}\")\n",
    "\n",
    "            if os.path.isdir(folder1):\n",
    "                folder_path = folder1\n",
    "            elif os.path.isdir(folder2):\n",
    "                folder_path = folder2\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            path1 = os.path.join(folder_path, file1)\n",
    "            path2 = os.path.join(folder_path, file2)\n",
    "\n",
    "        if not (os.path.exists(path1) and os.path.exists(path2)):\n",
    "            continue\n",
    "\n",
    "        # Procesar archivos\n",
    "        raw_code1 = analyzer.read_java_file(path1)\n",
    "        raw_code2 = analyzer.read_java_file(path2)\n",
    "\n",
    "        cleaned_code1 = analyzer.clean_code(raw_code1)\n",
    "        cleaned_code2 = analyzer.clean_code(raw_code2)\n",
    "\n",
    "        tokens1, success1 = analyzer.tokenize_code_advanced(cleaned_code1)\n",
    "        tokens2, success2 = analyzer.tokenize_code_advanced(cleaned_code2)\n",
    "\n",
    "        if not tokens1 or not tokens2:\n",
    "            continue\n",
    "\n",
    "        # Calcular similitudes\n",
    "        tfidf_sim = enhanced_tfidf_similarity(tokens1, tokens2, dataset)\n",
    "\n",
    "        ast_features1 = analyzer.extract_ast_features(cleaned_code1)\n",
    "        ast_features2 = analyzer.extract_ast_features(cleaned_code2)\n",
    "        ast_sim = analyzer.compute_intelligent_ast_similarity(\n",
    "            ast_features1, ast_features2, dataset\n",
    "        )\n",
    "\n",
    "        # Similitud combinada adaptativa\n",
    "        if dataset == 'conplag':\n",
    "            # Para ConPlag: m√°s peso a TF-IDF\n",
    "            combined_sim = 0.85 * tfidf_sim + 0.15 * ast_sim\n",
    "        else:  # ir_plag\n",
    "            # Para IR-Plag: balance 70-30\n",
    "            combined_sim = 0.70 * tfidf_sim + 0.30 * ast_sim\n",
    "\n",
    "        # Caracter√≠sticas adicionales para ML\n",
    "        length_ratio = min(len(tokens1.split()), len(tokens2.split())) / max(len(tokens1.split()), len(tokens2.split()), 1)\n",
    "        complexity_ratio = min(ast_features1.get('cyclomatic_complexity', 1), ast_features2.get('cyclomatic_complexity', 1)) / max(ast_features1.get('cyclomatic_complexity', 1), ast_features2.get('cyclomatic_complexity', 1))\n",
    "\n",
    "        resultado = {\n",
    "            'folder': os.path.basename(folder_path),\n",
    "            'file1': os.path.basename(path1),\n",
    "            'file2': os.path.basename(path2),\n",
    "            'tfidf_similarity': round(tfidf_sim, 4),\n",
    "            'ast_similarity': round(ast_sim, 4),\n",
    "            'combined_similarity': round(combined_sim, 4),\n",
    "            'length_ratio': round(length_ratio, 4),\n",
    "            'complexity_ratio': round(complexity_ratio, 4),\n",
    "            'es_plagio': plagio,\n",
    "            'dataset': dataset,\n",
    "            'tokenize_success': success1 and success2,\n",
    "            'ast_success': ast_features1['ast_success'] and ast_features2['ast_success']\n",
    "        }\n",
    "\n",
    "        resultados.append(resultado)\n",
    "        processed += 1\n",
    "\n",
    "        if processed % 100 == 0:\n",
    "            print(f\"‚úÖ Procesados {processed}/{len(df)} pares...\")\n",
    "\n",
    "    # Guardar y analizar resultados\n",
    "    df_resultados = pd.DataFrame(resultados)\n",
    "    df_resultados.to_csv(output_csv, index=False)\n",
    "\n",
    "    print(f\"\\nüìä RESULTADOS OPTIMIZADOS:\")\n",
    "    print(f\"‚úÖ Pares procesados: {len(df_resultados)}\")\n",
    "\n",
    "    # An√°lisis por dataset\n",
    "    for dataset in ['ir_plag', 'conplag']:\n",
    "        subset = df_resultados[df_resultados['dataset'] == dataset]\n",
    "        if len(subset) > 0:\n",
    "            plagiados = subset[subset['es_plagio'] == 1]\n",
    "            no_plagiados = subset[subset['es_plagio'] == 0]\n",
    "\n",
    "            print(f\"\\nüìà {dataset.upper()}:\")\n",
    "            if len(plagiados) > 0:\n",
    "                print(f\"  Plagiados - TF-IDF: {plagiados['tfidf_similarity'].mean():.3f}\")\n",
    "                print(f\"  Plagiados - AST: {plagiados['ast_similarity'].mean():.3f}\")\n",
    "                print(f\"  Plagiados - Combinado: {plagiados['combined_similarity'].mean():.3f}\")\n",
    "            if len(no_plagiados) > 0:\n",
    "                print(f\"  No plagiados - TF-IDF: {no_plagiados['tfidf_similarity'].mean():.3f}\")\n",
    "                print(f\"  No plagiados - AST: {no_plagiados['ast_similarity'].mean():.3f}\")\n",
    "                print(f\"  No plagiados - Combinado: {no_plagiados['combined_similarity'].mean():.3f}\")\n",
    "\n",
    "    print(f\"\\nüíæ Resultados guardados en: {output_csv}\")\n",
    "    print(df_resultados.head())\n",
    "\n",
    "    return df_resultados\n",
    "\n",
    "def evaluate_thresholds(df_results: pd.DataFrame):\n",
    "    \"\"\"Eval√∫a diferentes thresholds para encontrar el √≥ptimo\"\"\"\n",
    "    print(f\"\\nüéØ EVALUACI√ìN DE THRESHOLDS\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    features = ['tfidf_similarity', 'ast_similarity', 'combined_similarity']\n",
    "    thresholds = np.arange(0.3, 0.8, 0.05)\n",
    "\n",
    "    best_results = {}\n",
    "\n",
    "    for feature in features:\n",
    "        best_acc = 0\n",
    "        best_thresh = 0\n",
    "\n",
    "        print(f\"\\nüìä {feature.upper()}:\")\n",
    "        for thresh in thresholds:\n",
    "            predictions = (df_results[feature] > thresh).astype(int)\n",
    "            accuracy = (predictions == df_results['es_plagio']).mean()\n",
    "\n",
    "            if accuracy > best_acc:\n",
    "                best_acc = accuracy\n",
    "                best_thresh = thresh\n",
    "\n",
    "            print(f\"  Threshold {thresh:.2f}: Accuracy {accuracy:.3f}\")\n",
    "\n",
    "        best_results[feature] = {'threshold': best_thresh, 'accuracy': best_acc}\n",
    "        print(f\"  üèÜ Mejor: {best_thresh:.2f} (Accuracy: {best_acc:.3f})\")\n",
    "\n",
    "    return best_results\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BASE_PATH = 'data/splits/train'\n",
    "    CSV_PATH = 'data/splits/train.csv'\n",
    "\n",
    "    # Procesamiento optimizado\n",
    "    df_results = process_dataset_optimized(BASE_PATH, CSV_PATH)\n",
    "\n",
    "    # Evaluaci√≥n de thresholds\n",
    "    best_thresholds = evaluate_thresholds(df_results)"
   ],
   "id": "85807374b2ff9c9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ PROCESAMIENTO OPTIMIZADO DEL DATASET\n",
      "=============================================\n",
      "üìä Procesando 957 pares...\n",
      "‚úÖ Procesados 100/957 pares...\n",
      "‚úÖ Procesados 200/957 pares...\n",
      "‚úÖ Procesados 300/957 pares...\n",
      "‚úÖ Procesados 400/957 pares...\n",
      "‚úÖ Procesados 500/957 pares...\n",
      "‚úÖ Procesados 600/957 pares...\n",
      "‚úÖ Procesados 700/957 pares...\n",
      "‚úÖ Procesados 800/957 pares...\n",
      "‚úÖ Procesados 900/957 pares...\n",
      "\n",
      "üìä RESULTADOS OPTIMIZADOS:\n",
      "‚úÖ Pares procesados: 957\n",
      "\n",
      "üìà IR_PLAG:\n",
      "  Plagiados - TF-IDF: 0.678\n",
      "  Plagiados - AST: 0.866\n",
      "  Plagiados - Combinado: 0.734\n",
      "  No plagiados - TF-IDF: 0.667\n",
      "  No plagiados - AST: 0.885\n",
      "  No plagiados - Combinado: 0.732\n",
      "\n",
      "üìà CONPLAG:\n",
      "  Plagiados - TF-IDF: 0.584\n",
      "  Plagiados - AST: 0.894\n",
      "  Plagiados - Combinado: 0.631\n",
      "  No plagiados - TF-IDF: 0.423\n",
      "  No plagiados - AST: 0.722\n",
      "  No plagiados - Combinado: 0.468\n",
      "\n",
      "üíæ Resultados guardados en: similitud_optimized.csv\n",
      "              folder          file1          file2  tfidf_similarity  \\\n",
      "0  c57a973e_fa484fdd  c57a973e.java  fa484fdd.java            0.3820   \n",
      "1  44428e63_c850e422  44428e63.java  c850e422.java            0.4771   \n",
      "2  2ff0355e_83935617  2ff0355e.java  83935617.java            0.4478   \n",
      "3  0017d438_9852706b  0017d438.java  9852706b.java            0.4633   \n",
      "4  bdfe8110_c57a973e  bdfe8110.java  c57a973e.java            0.3879   \n",
      "\n",
      "   ast_similarity  combined_similarity  length_ratio  complexity_ratio  \\\n",
      "0          0.7127               0.4316        0.9347            0.7500   \n",
      "1          0.8162               0.5280        0.6466            1.0000   \n",
      "2          0.5591               0.4645        0.6274            0.5625   \n",
      "3          0.9104               0.5303        0.8854            1.0000   \n",
      "4          0.8741               0.4608        0.9205            0.7500   \n",
      "\n",
      "   es_plagio  dataset  tokenize_success  ast_success  \n",
      "0          1  conplag              True         True  \n",
      "1          0  conplag              True         True  \n",
      "2          0  conplag              True         True  \n",
      "3          1  conplag              True         True  \n",
      "4          1  conplag              True         True  \n",
      "\n",
      "üéØ EVALUACI√ìN DE THRESHOLDS\n",
      "------------------------------\n",
      "\n",
      "üìä TFIDF_SIMILARITY:\n",
      "  Threshold 0.30: Accuracy 0.462\n",
      "  Threshold 0.35: Accuracy 0.502\n",
      "  Threshold 0.40: Accuracy 0.575\n",
      "  Threshold 0.45: Accuracy 0.658\n",
      "  Threshold 0.50: Accuracy 0.717\n",
      "  Threshold 0.55: Accuracy 0.708\n",
      "  Threshold 0.60: Accuracy 0.689\n",
      "  Threshold 0.65: Accuracy 0.675\n",
      "  Threshold 0.70: Accuracy 0.673\n",
      "  Threshold 0.75: Accuracy 0.661\n",
      "  üèÜ Mejor: 0.50 (Accuracy: 0.717)\n",
      "\n",
      "üìä AST_SIMILARITY:\n",
      "  Threshold 0.30: Accuracy 0.442\n",
      "  Threshold 0.35: Accuracy 0.442\n",
      "  Threshold 0.40: Accuracy 0.442\n",
      "  Threshold 0.45: Accuracy 0.444\n",
      "  Threshold 0.50: Accuracy 0.448\n",
      "  Threshold 0.55: Accuracy 0.466\n",
      "  Threshold 0.60: Accuracy 0.485\n",
      "  Threshold 0.65: Accuracy 0.541\n",
      "  Threshold 0.70: Accuracy 0.618\n",
      "  Threshold 0.75: Accuracy 0.682\n",
      "  üèÜ Mejor: 0.75 (Accuracy: 0.682)\n",
      "\n",
      "üìä COMBINED_SIMILARITY:\n",
      "  Threshold 0.30: Accuracy 0.447\n",
      "  Threshold 0.35: Accuracy 0.462\n",
      "  Threshold 0.40: Accuracy 0.503\n",
      "  Threshold 0.45: Accuracy 0.601\n",
      "  Threshold 0.50: Accuracy 0.690\n",
      "  Threshold 0.55: Accuracy 0.755\n",
      "  Threshold 0.60: Accuracy 0.746\n",
      "  Threshold 0.65: Accuracy 0.703\n",
      "  Threshold 0.70: Accuracy 0.692\n",
      "  Threshold 0.75: Accuracy 0.674\n",
      "  üèÜ Mejor: 0.55 (Accuracy: 0.755)\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
